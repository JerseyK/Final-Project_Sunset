{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f92af7-a516-49b7-ba84-75ccb912a52d",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0cf09-d260-408e-843d-1ae76600fcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from eda import insufficient_but_starting_eda\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276c60-d00a-4502-bbeb-bde4a7e31aeb",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73c976-c659-49b9-aa31-7ea68c57f271",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f82622-d959-43ed-9abc-7054874e2a59",
   "metadata": {},
   "source": [
    "We used three different datasets to create one final dataset\n",
    "\n",
    "[S&P 500](https://github.com/JerseyK/Final-Project_Sunset/blob/d3a36fde0bb19d897fb15effcb85ffb0f04ec78b/inputs/sp500_2022.csv): this dataset was used as we narrowed in on the companies we would look at for this project. We scraped this dataset from Wikipedia\n",
    "\n",
    "[Compustat Customer Supplier](https://github.com/JerseyK/Final-Project_Sunset/blob/d3a36fde0bb19d897fb15effcb85ffb0f04ec78b/inputs/cust_supply_2019_2022.csv): this dataset contained raw data provided by Dr. Bowen that showed filings between customers and suppliers between years 2019 and 2022\n",
    "\n",
    "[Accounting 2018-2022](https://github.com/JerseyK/Final-Project_Sunset/blob/d3a36fde0bb19d897fb15effcb85ffb0f04ec78b/inputs/acct_data.csv): this dataset was also provided by Dr. Bowen and is comprised of accounting variables we requested based off of the list of firms we found through EDA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e01935-d478-4089-a366-2ceba1e7af03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "comp = pd.read_csv('inputs/cust_supply_2019_2022.csv')\n",
    "sp500 = pd.read_csv('inputs/sp500_2022.csv')\n",
    "acct_raw = pd.read_csv(\"inputs/acct_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd0eaf-9498-44dc-b850-ab4c5d77ea03",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7c2fd-a83e-4ea5-8d1b-d6c938462ca1",
   "metadata": {},
   "source": [
    "**S&P 500**: No data cleaning necessary\n",
    "\n",
    "**Compustat Customer Supplier**: We first ran [EDA](https://github.com/JerseyK/Final-Project_Sunset/blob/d3a36fde0bb19d897fb15effcb85ffb0f04ec78b/data_eda.ipynb) on the raw to gain a better undertanding of the data before we cleaned it. From that we found that\n",
    "- there are 77901 data entries in this csv\n",
    "- there are 9 categorical variables\n",
    "- there are 6 numerical variables\n",
    "- the unit level is sales\n",
    "- the only variables with missing data are \n",
    "    - gareac (57.8%) \n",
    "    - gareat (57.8%)\n",
    "    - stype (14.0%)\n",
    "    - salecs (12.4%)\n",
    "    - cik (0.6%)\n",
    "\n",
    "\n",
    "We also found that even though more than 50% of geograpic area code (gareac) and geograpic area type (gareat) are missing, these values wouldn't be needed for our analysis and would be dropped. We are also were not concerned about segment type (stype) as we used gics sector instead to describe the seller. Anoteher key finding that wasn't of too much conern was that 12.4% of that sales was blank. Often sales (salecs) are not reported when the buyer is also \"not reported\", therefore it didn't concern us.\n",
    "\n",
    "Based off of our EDA, the first thing we needed to do was drop all the observations where the customer name was not reported. Next we dropped any observations where there were no sales reported. We then took the remaining observations, renamed `cik` to become `CIK` to be able to merge with the S&P 500 data later. After that merge we dropped all firms that were not in the years 2019 or 2022. We also made sure that the firms remaining appeared in both 2019 and 2022. That gave us a list of 355 unique `gvkey` which we then got accounting data for.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For this dataset we are going to say that any firm filing in 2019 corresponds to information in 2019 fiscal year. We know that this can lead to some inaccuracies when firms don't file in 2019 for 2019 fiscal year. For instance if a firm files in January of 2020 our analysis is that this data will correspond with the fiscal year of 2020, wehn in reality the data actually corresponds with 2019 fiscal year.\n",
    "- TLDR: the fiscal year = the filing year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d9726-a6bf-48f9-8811-e781b6292959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comp2 = comp\n",
    "comp2 = comp2[comp2['cnms'] != 'Not Reported']\n",
    "comp3 = comp2.dropna(subset=['salecs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a71c2f-ffe7-43ec-bb8c-de958a5438a6",
   "metadata": {},
   "source": [
    "**Accounting 2018-2022**: For the accounting dataset we used the list of unique `gvkey` to idenifty the firms we wanted accounting data for. We provided Dr. Bowen with that list of keys along with the rest for the following accounting variables:\n",
    "- fyear (fiscal year)\n",
    "    - sale (net sales)\n",
    "    - rect (receivables/total)\n",
    "    - invt (inventories)\n",
    "    - ap (accounts payable - trade)\n",
    "    - ib (income before extraordinary items)\n",
    "    - ni (net income (loss))\n",
    "    - obidp (operating income before depreciation)\n",
    "    - at (total assets)\n",
    "    - capx (capex, dollar amount)\n",
    "    - capxv (capex ratio for current fiscal year)\n",
    "    - cogs (cost of goods sold)\n",
    "    - gp (gross profit)\n",
    "    - epsfx (eps basic (takes into account the actual number of shares outstanding, and does not include any potentially dilutive securities))\n",
    "    - acominc (net income)\n",
    "    \n",
    "With this new dataset we then performed [EDA](https://github.com/JerseyK/Final-Project_Sunset/blob/d3a36fde0bb19d897fb15effcb85ffb0f04ec78b/data_eda.ipynb) to understand it as a whole. After running that we found that\n",
    "- there are 26905 obersvations\n",
    "- the unit level is firm year\n",
    "- all variables are numerical as we requested them        \n",
    "    - there are 16 variables\n",
    "- the variables with missing data are\n",
    "    - capxv (16.3%)\n",
    "    - oibdp (11.4%)\n",
    "    - invt (8.9%)\n",
    "    - capx (8.7%)\n",
    "    - rect (8.5%)\n",
    "    - acominc (8.3%)\n",
    "    - ap (8.3%)\n",
    "    - epsfx (8.1%)\n",
    "    - ib (8.0%)\n",
    "    - ni (8.0%)\n",
    "    - cogs (8.0%)\n",
    "    - gp (8.0%)\n",
    "    - sale (8.0%)\n",
    "    - at (7.7%)\n",
    "    \n",
    "    \n",
    "Based off of our EDA we decided to use `ni` as the variable to represent net income as it had the least missing values. We used the same logic to decide to use `capx` over `capxv`. In turn we were then able to drop the other variables that represented net income `acominc`, `oibdp`, and `ib`, as well as `capxv`. After dropping those variables we had a dataset ready that we could add to by creating variables to show the growth (return) between 2019 and 2022 firms for each accouting variables before merging with the SP & 500 and Compustat dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c580c82-5d03-46d9-8454-75ba7a5137f6",
   "metadata": {},
   "source": [
    "## Merging the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e6004-5a9a-46fb-bd6c-095d57b4cee5",
   "metadata": {},
   "source": [
    "### Merging Compustat & SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09aa23-7899-4252-89c6-74f7bf3a645f",
   "metadata": {},
   "source": [
    "We merged `comp` with `sp500` to create `merged` which merged the two on `CIK`. This final dataset gave us 385 unique firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f7715-7462-4743-b536-9a291d70b853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comp3 = comp3.rename(columns = {'cik': 'CIK'})\n",
    "merged = comp3.merge(sp500, on='CIK', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f364f2-6d70-487c-b248-ef4445f3a0e1",
   "metadata": {},
   "source": [
    "Next we dropped the filings that were not in 2019 or 2022. We used the indicies of the filtered dates (2020-01-01 to 2021-12-31) to be be dropped. This left us with 355 unique firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8875402-c0d3-4187-a2a8-1f5eb39bca10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged['date'] = pd.to_datetime(merged['srcdate'])\n",
    "dates = merged.sort_values(by='srcdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b2612-844f-4e4c-bb5f-dd1b7d714f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2021-12-31'\n",
    "filtered_df = merged.query('@start_date <= date <= @end_date')\n",
    "\n",
    "filtered_indices = filtered_df.index\n",
    "\n",
    "filtered_out_df = merged.drop(filtered_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae75a2-33b4-4d70-9847-d79d96f3d27f",
   "metadata": {},
   "source": [
    "Since we are assume that the firm year is the fiscal year we need to make sure that year standardized amongst our data. Lastly we then filtered out any firms that didn't have filings showing in 2019 <em>and</em> 2022. This left us with 89 unique firms to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b408d9-13a7-42f7-8d39-9c6a7cc215b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_out_df['fyear'] = pd.to_datetime(filtered_out_df['srcdate']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a62e34-829b-44f8-a27b-4d426b6abe38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = filtered_out_df.groupby('gvkey').filter(lambda x: x['fyear'].max() == 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f6194-8ff8-48e9-a582-ecf10ac96451",
   "metadata": {},
   "source": [
    "### Identifying the Unique `gvkey` to Obtain Accounting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97eef4-fe8b-4f34-82ec-b8f6566d1db2",
   "metadata": {},
   "source": [
    "With the `filtered` dataset we were able to extract the list of unique `gvkey` to a csv file to pass along to Dr. Bowen to get the accounting data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f6680-a352-4aa9-9c31-44c9af8b3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "listkeys = pd.DataFrame(filtered_out_df['gvkey'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22a4bc-b793-4827-bbcd-977ccbc42501",
   "metadata": {},
   "source": [
    "### Merging Accounting Data & SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa0c95-6f47-40b0-87e6-d0ca4d68d7ae",
   "metadata": {},
   "source": [
    "With the cleaned accounting data we were able to merge `listkeys` and `acct_raw` on `gvkey`. There were had 354 unique firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c7289-e961-47bc-98d9-00e93114ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "listkeys2 = listkeys.rename(columns={0: 'gvkey'})\n",
    "\n",
    "merged_acct_raw_keys = pd.merge(listkeys2,acct_raw,how = 'inner', on='gvkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9193378-4f5f-4b44-92e4-267853421e1d",
   "metadata": {},
   "source": [
    "The next step was to take that new merged dataframe and filter it to `fyear` to be 2019 or 2022. We then dropped the variables we deemed unncessary during data cleaning to make our dataset concise and to only have variables we would be using for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e4360-669c-4131-be45-6f9a45db084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_df = merged_acct_raw_keys.query('fyear == 2019 or fyear == 2022')\n",
    "acct_df = acct_df[['gvkey', 'fyear', 'ap', 'at', 'capx', 'cogs', 'epsfx', 'gp', 'invt', 'ni', 'rect', 'sale']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f2b3d-3656-4802-b83e-4e52b01130a3",
   "metadata": {},
   "source": [
    "The final step that need to be taken before merging with the `filtered` dataset was creating additional variables. We created variable for each accouting variable (`ap`, `capx`, `ni`, etc) to show the growth between 2019 and 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb738011-db74-41d9-85c8-4a4197270926",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_row = acct_df.iloc[0]\n",
    "for index, row in acct_df.iloc[1:].iterrows():\n",
    "    if row.values[1] == 2019.0:\n",
    "        prev_row = row\n",
    "    elif row.values[1] == 2022.0:\n",
    "        calc_row = (row - prev_row) / prev_row * 100\n",
    "        calc_row.rename({k: f\"calc_{k}\" for k in calc_row.index}, inplace=True)\n",
    "        acct_df.loc[index, calc_row.index] = calc_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d2bfc-c0d9-46e4-b6f5-11c1ea307bee",
   "metadata": {},
   "source": [
    "## The Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d9c35-b704-429e-ab0b-feaa94709608",
   "metadata": {},
   "source": [
    "To create our final dataset to be used for our dashboard and analysis we needed to merge the compustat dataset and the accouting dataset together. To do so we took `filtered` and kept our desired columns of `gvkey`, `fyear`, `conm`, `Symbol`, `CIK`. The next thing we did was drop duplicates so that we could be able to match firms in the accoutning dataset. In the final merge we were left with 89 unique firms. We then used `to_csv` to save that dataset to use for the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f97f0-efd3-4d8b-a680-7bfb06a82e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_comp = filtered[['gvkey', 'fyear', 'conm', 'Symbol', 'CIK']]\n",
    "cleaned_comp = cleaned_comp.drop_duplicates()\n",
    "\n",
    "accounting = pd.merge(cleaned_comp,acct_df, how='left',on=['gvkey','fyear'], indicator=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
